I would turn the order of this around, it would make the motivation clearer. The introduction should also be much longer with linguistic examples and citations of related work (but I see that you want to keep it for later.)

Our goal is to study referring, how we refer to entities and have we interpret the entities referred to. 

There is a lot of ambiguity involved as language only maps to the world in an under specified way.  The first problem is that a word may map to several pixels. There is a loss of information/abstraction. The second problem is that referring expressions are under-specified, chair can be any of the 5 chairs. This is to compress information in communication, we say less than we mean. Illustrate both points with examples.

Instead, humans rely on communicative protocols to disambiguate referring expressions. The Dale and Reiter algorithm and the literature on GRE. Communicative protocols are established through language games, some parts seen to be universal, some parts are negotiated on the fly.

Artificial language games - describe in more detail what they are and how they are implemented - allow us to study language with8n this communicative setting. 

In this thesis we look at referring within the context of communicative games to explore both theoretical and practical (computational) limits of grounded referring expressions in interactive setting.

The novelty of this thesis is that we study referring to entities through language games involving sequences of descriptions.

What are the limits of the agent architecture and and input representation on learning successful grounding referring expressions through language games?

To what degree does the emergent referring expressions align with referring expressions in a natural language such as English what constraints can be imposed on the environment and the agents themselves that languages align?

Here we need to cite: (1) literature on grounding, connecting language and vision, Harnad, Roy, us, etc. (2) referring expression generation, Dale and Reiter, (4) reference and co- reference (Poesio, us), (5) language games (Wittgenstein) and referring as a collaborative process (Clark, David Lewis), (6) language games (work by Kazakov and Kirby), within robotics (Steels), language games within neural models (Baroni).

Mathias’ work is also relevant https://era.ed.ac.uk/handle/1842/38727

We also know the ground truth about the scenes as we know the function that generated them.

We can control the properties of these datasets and we can introduce as much bias and ambiguity as we see firm in each experiment to compare with the natural datasets.

Which is how this is done in practice, cf. referring as a collaborative process, the paper by Clark.

What are those?

What kind of complexity?

Leave for the last chapter, limitations and future work.

Not just real world, also to background knowledge and what the person wants to do. 

Grounding is more related to interpretability of expressions rather than intent. Without grounding they cannot be interpreted. The intent shapes WHAT referring expressions are generated in what contexts of the real world.   Hence, intent is a meta thing connecting world, language, knowledge and agent’s experience.

They learn how to interpret expressions against some representation. Hence, a reference is a relation between a description and some entity, it’s interpretation. Without this connection referring expressions are useless as we do not know how to interpret this. Textual models might be good for general knowledge such as factual information from news and Wikipedia but as soon as we have NLP applications that interact with other domains, e.g. a ticket booking system or a situated robot involved in patient care, we need to connect language to some other representations to make language interpretable.

Give an example of a language games. Symbols are invented at random. However, the constraints of interaction control how new symbols are introduced and when existing symbols are re-used and used. Agents cannot invent unlimited number of symbols to refer to every event the6 encounter as they have limited memory and therefore they are driven by abstraction. On the other hand, the symbols must be flexible enough. The other agent must be able to resolve the symbols they hear based on the context. Hence, a successful interaction is when a describer is producing such symbols that interpreter can easily interpret within the context, here an image. This is also how a reward function is defined and loss is propagated.

Rephrase, language games provide interactive constraints within which language can emerge. But what are these? In this thesis we will study some, such as agent’s memory and layer representation and the feature representation, the structure of the world and its ambiguity.

Because referring expressions are ambiguous participants rely on language games to resolve the reference in context.

Give an example of a game?

Examples of architectures from Baroni. The EGG framework.

Rather, being an artificial dataset, it allows us to precisely control the bias and therefore explore its limits.

We know the ground truth function that generated the scenes and hence we can also make predictions about different effects of contexts.

We will use the code to generate a new dataset of scenes and descriptions with the properties we want to study.

You are not just taking their dataset but you are taking the framework and you are applying it on a new task, to generate a new dataset(s) with carefully controlled properties. The CLEVR framework is suitable for this because…

Reference to GitHub, also perhaps in the following text when you describe your own code, it would be good to include a reference to the code as you go along.

For this, functions from the existing code are used, right?

Give also example of ground truth features that were used to generate this, i.e. objects and attributes.

What are the differences? Mainly in the domain of objects and scenes. What data are VGG19 and ResNet trained on?

Link to the discussion on what datasets the systems are trained on.

This is interestingly more complex and deserves much longer discussion. Pre-trained knowledge may be unhelpful if it is very different from the new domain. Hence, at some point training from scratch may be more successful. 

What is success? Learning may take longer but then after some time the performance is better than with pre-training. 

Since agents are free to invent new language it is not clear whether this will be align with the human language that influenced pre-training the visual models, hence there is both effect of domain and language here. 

We should make these questions part of a discussion, particularly if we are comparing the effects of pre-training vs training from scratch in the experiments.

What is the framework? We saw that language games can be implemented very differently, with robots, as code, etc. it is a neural model.

More detailed description of all these. Readers will not be familiar with Gumbel-Softmax for example.

You frequently start discussion by referring to quite specific concepts straight away in a very concise way, e.g. Gumbel Softmax, Reinforce, without defining them but then an explanation comes later in the text. The problem with this a reader would wonder what they are and would look for explanation. It is best to have a very general introduction, e.g. we will test two optimisation functions and then introduce them and explain them in one go later.

:-)

This is what a technical manual would say - but what does it do really? What is the difference between reinforce and Gumbel softmax in practice? In our case? Reinforce is still superior but here you make it less.

Include a diagram?

This sounds like details of your implementation and should come later. Here, we would only need a general description of the EGG framework. Also for this text later, it would be good to have a diagram of all these steps and examples what these games actually are.

to study biases

The work can on the long run mitigate harm as it is provides a study of models, how they would behave on real data and therefore contributes towards interpretability of AI.

Capitalisation

Too informal

We create a new dataset based on the CLEVR framework where we control the appearance of scenes in a referring expression task.

The scenes are controlled by human-recognisable attributes of object such as object shape, colour and type. These attributes also correspond to referring expressions in natural language such as English.

We create two contexts of scenes, one with two objects and one with five. 

We also control for the number of attributes shared between the target and the distractor.

To control how challenging the referring task is

Introduce other images earlier?

It depends what you mean by distractor objects. I’d say they are all distractor objects by the virtue they share attributes. The system has to learn understanding of an intersection of these attributes.

Note that since objects are generated as a part of 3-d scenes, they may appear differently on images: size, occlusion, shading, rotation. We also expect the model to learn from this noise and which therefore makes the task much harder and a task that approximates natural environment compared to the task where we would use abstract geometric shapes projected on a 2-d plane.

But why not test the limits and the harder examples?

Wouldn’t it make sense to introduce the discussion of how scenes can be generated with attributes somewhere here? Again, I felt we were projecting forward earlier without understanding the task.

In order words, the choice of attributes is random for distractor and there may be an overlap between the same distractor objects. This is because we do not control overlap attributes over distractor, I.e. random.

In the previous dataset the target and distractor are discriminated by ONE attribute exactly.

Vague

In real world, objects have over-lapping attributes and hence a single object can only be identified by an intersection of attributes.

However, we do not do this randomly but ge5 inspiration from the Dale and Reiter generation algorithm who observe that attributes in descriptions occur in certain order and are added incrementally in a certain hierarchy. This way we approximate the information in the scenes to human cognition and we hope that the system will be able to exploit that (we would really need to study and compare it with another dataset where attributes are add3d randomly to confirm that there is an effect of h7man. Ignition).

The sharing of attributes should be according to the Dale and Reiter hierarchy, shouldn’t it? Two attributes, big blue (cube|sphere)

It’s actually the reverse in which descriptions are generated, we start left to right: large purple (are shared), the last attribute must thus be unique. Generation proceeds in the opposite order, one would just say sphere. We should clarify this. Now it is very hard to understand.

This should go at the beginning where language games with neural agents are introduced.

We

This is a natural language understanding or reference resolution task.

Object identification task

Referring expression generation task

We are validating the dataset on 3 tasks

Reference resolution task

Reference resolution with identification of location. The other task, object identification is also resolving reference but it is easier as you are only picking objects. The first task also involves spatial knowledge.

What are the features?

Note that these are visual features and not spatial features. It is true that visual features also encode some spatial information, how visual features relate to each other, but such information is very different from the spatial information required to predict coordinates in a coordinate frames, and hence we expect the task will be very hard.

Very precisely. This is higher resolution that attention in the visual models that operates on o 7x7 grid, normally.

How are these encoded?

Encoded locations?

Convolutions are only applied on visual features, not object attribute features and locations.

Why shuffled? If we order them the way they appear in the image, the model will have more information that they are sequentially related.

I don’t understand.

The question you are asking is whether it is possible to predict location from the visual appearance of the object. This is highly complex task as it requires quite two step reasoning, identification of features with that attribute, e.g. blue and then locating that feature in the image.

See my point earlier about the reversed algorithm between the two tasks 

\citep and \citet Use Name (year) when you are referring to particular person and (Name, year) when you are referring to a paper. Hence, in this case it would be the latter.

Oh, before we talked about 3 methods

The information in the masked image is still not enough to identify the precise location, only the region. Hence, it corresponds on attention in the attention models. But those are generating labels and not predicting coordinated. The task is still challenging.

Yes that’s correct but through several epochs we hope we will refine the distance and standard deviation of the error. It should level out. It is not a problem. What you do with a circle and accuracy is that you make the task easier as your pointer is now not pointing to a point but to a larger area.

But here the score will face the same problem with steward deviation. Hence the only difference here is that pointing is less precise.

Explain. w3 had a paper with John Kelleher on what spatial information is encoded in CNNs

Point at not just discriminate

How can we have epochs on the testing data? There is no updates so i5 does not make sense.

The results show that the model is learning something from the training data but this is not the feature that should be learning as the performance on the test data is low. Actually, is it low, it is 10, 35 and 45 pixels. We should not expect any difference between epochs as there is no training. Hence, a flat line is expected.

Because it is in the same location?

Not true, given the previous observation. It is successful.

Not really. The geometric information is impoverished, other info is rich here, the models still perform to a point.

What is that?

What is the image size? What is the size of a typical object? 50 sounds quite good. Attention is predicting one of 7x7 blocks. Is 50 more or less than 1/7 of an image? 

Explanation of labels

Perhaps arranging the objects sequentially would help the model learn spatial contiguity, see my earlier comment.

Table with these results?

More explanatory caption

But this image does not show clearly whether the circles for the same object match. You could calculate the average error in distance between the ground truth and predicted coordinates.

They are closer to the target than distractor and you can see that it is working to a point. But remember this task is very challenging and I wouldn't say the results are so negative.

There is a bias towards the centre but I would not say that the model has not learned anything. I'm surprised that it works so well given the feature representation we have, i.e. no geometric features.

The centre bias could be the way the error function is used, that is averages all distances and of course this has a tendency to some middle distance. A solution would be to have better geometric features that could take these errors better into account.

centre - using British English?

There is a centre bias and the model can partially locate the target. The task is hard. The features are not optimal for this task. Future would should focus on using geometric features - we should have tried these and I'm sure we would get better results. Another extension would be making pointing less precise, i.e. focus on a single point, i.e. using a circle of 20 pixels or a 7x7 grid as in the attention mechanism. This would simplify the task.

I Haven't checked the earlier captions, but the captions should be informative in the sense that one does not need to look for the text to understand them. Hence, include a brief summary of what each figure contains.

Just say that this is a task of identifying objects (the previous was about locating them). This task does not rely on geometric information but it relies on identifying correct object features, hence explore different features and hence it is useful as abenchmarking task.

I haven't commented on this before, but let us be very careful in naming all meta-paramaters of the models in the text or in caption: how many layers, how many dimensions, etc. The reader should be able to replicate the code and the experiment from this. You can do this in the caption which is a longer explanation or in the text itself.

This is quite low? Why only 10?

So, the output of this model is a vector of 0 and 1s, with the 1 identifying the corrct object (or the other way around?).

I don't understand this? Have you actually implemented such model? But how can you train a random classifier? The baseline would normally be 1/5 or 1/5, i.e. the number of labels if the number of examples is equal, other a weighted proportion of these. By definition of "random" you would expect such classifier to achieve these probabilities.

Why not?

The model is able to leatn to discriminate objects based on visual appearance.

Well, this experiment is a natural language generation task: whether the model is able to generate referring expressions.

Slots have the same semantic value, i.e. the last slot is the object type. We can do this because we know the structure of the descriptions in this dataset and because we hope this will help the learning. Also, we are thinking of generation based on slots.

We... I sounds more like you are writing a lab report with casual language but here you want to be more formal.

In this case it should be using the same attributes as humans as we are training it on human labels.

Dimensions?

cross-entropy

Why is this teacher forcing?

There is a greedy decoding algorithm.

You add attention to the target object.

This is normally implemented as a filter of 1 and 0 corresponding to pixels. As such it then corresponds to attention, cf. paper with Mehdi.

This is too strict. if this is a generation task then we can use generation metrics. Those compare if the generated string as a whole is kind of similar to the target string, see ROUGE, METEOR, BERTscore.If this is a classification task, and it is, given that we know what type of word we expect in each slot, we could calculate accuracy per class, precision, recall, F-score. We could also calculate mean ranks for predicting words per classes, or acucracy at k- if the target word is in the k-top predictions.We could also take it a multi-class classification problem and how to calculate accuracy in this case, would need to check.

Too far away, might be worth repeating here, you say still stay Fiigure 2d, repeated asFigure 6 here

Is this figure for a particular class, i.e. object type, or do you then do something to individual accuracy, i.e. average them?

Very good If I understand this right this would be a case where the generated description describes another object better than the target and a human would identify that. So it is like false negatives in describing. Perhaps use another term for this: referring to a distractor? (since you are not measuring hwo individually the description fitts a distractor but only if the entire description could perfectly fit a distractor)

are

Not shown in Table 5

But there is an importnat difference that now we have two neural netweorks, the sender and the receiver.

Not sure. Note that <pad> is a token. The model has to learn when to say <pad> just as any word, hence knowing when not to dewscribe is also trickly.

I think we need individual accuracies per class and also precision, recall and the f-score. If we include the joint accuracy figure when you also need to explain how you avergaed the infividual accuracies.

Masking does not work but perhaps this is because you applied convolutional pre-trained filters rather than 0-1. It is harder to detect meaningful patterns since the targets will be smaller and more integrated with other objects.

But this is no-surprise since we have fixed the slots.

Why 66? We have 3 attributes, right, so 33%?

?

I would present these for individual classes.

Not in the table?

Earlier you say that you will only try one method but now you have tried both. Change earlier text; also do you have full performance figures for this setup to include?

But before you argued, rather pesimistically, that having distractors with the same attributes actaully contributes to greater performance as it is moire likely that the correct word is generated. I would only pose this as a very vague hypothesis to be tested, as you conclude now it is also the case that images with simialr distractors are more difficult to describe correctly. Hence, it is by no way given.

Here it does not receive a boundig box of an object as in Lazaridou. They use whole images, don't they?

On the contrary,...

What is this setup?

Why are we not testing it on all 3 games?

Not sure what you mean here? Why do we loose human bias? 

But possibly you would describe the language game setup already before on the background or method chapter. I would move all text there and make the language games description much more detailed, also giving examples. What is happening in terms of interaction? How is this modelled with the neural networks? It is important to emphasise this difference.

Is this how they call it?

Deatils about the layers, their parameters. More explanatory figure caption.

Motivation why this is so. The possible number of strings is {#colour} x {#shape} {object type} where all numbers except the last one also contain a padding token.

This discussion of dimensions could already be introduced when you discuss simple modles earlier as the same dimensions are used. This is a more an evaluation of represnetations rather than interaction and hence it would fit there.

Calculation behind this figure?

Hypothesis space to find the target function is just too big and they can never approximate that function.

Not sure, since we never tested smaller layer sizes. If the embedding layers are the same as the vocabulary size then at least a language encoding LSTM is not motivated to geenralise at all as it can keep representations for individual words without abstracting them. Leanring word embeddings involves reducing dimensions.

Could we say that calculation on Dale-5 is successful?

How do we know? What would happen if the hidden layers are smaller than the vocabulary size? We would also need to test several smaller dimensions.

We would need to show that the messages in Dale-2 are shorter than in Dale-5 to claim this.

A note about the datasets chosen. Why do we choose these? Initially, we describe more than 3 datasets.

As we discussed at Semdial, it would be good to have a learning curves here that would show how fast or slow the languages converge. As David pointed out it may be that convergence is unstable and changes happen in between.

I don't understand how test and training set are used in these experiments. In language games one only has a training set and success is measured through the success in the game. It is not possible to have a test set as learning is continuous.

I don't understand this.

Is this controlled by the sender. The sender must have some policy how it is assigning messages. Or is it just random? But what makes the sender then to generate a single message vs a longer message. If it is not controlled - how could we implement some policies for message length?

Differences between training and testing configuration; what happens during testing, there is no learning involved but the systems see new images but they have to use the same vocabulary?

A table or a graph with these figures?

Are they? We haven't tested how humans would describe these scene. I see what you mean, but they you have to explain that you count as an English expression a description containing sequences of colour, size and type and then following the Dale and Reiter's algorithm.

All this has to do with the sender's policty how to generate messages. We should have varied this policy. I suspect no policy leads just to one message. How is the system motivated to generate more than one symbol?

But this is slightly problematic, since we strcutured the world this way to emphasise these attributes visually. Hence, a mapping would indicate that the symbols have good semantics if they can discriminate the colours. But this could have to do with the compisitionality issue, perhaps the system was biased to use single word expressions and hence this interfered with the grounding.

The probing test is to show how well the emerged language can be trasnlated into English descriptions consistting of labels of scene features using Dale and Reiter idea of incrementality. However, we would need to know now what was the policy of the sender to generate message. Was this similar to Dale and Reiter to enforce longer descriptions?

Move to the earlier paragraph when you introduce Dale and Reiter.

Baselines?

We

Is this encessary? But when and where is this loss using 0 inouts calculated?

I think at this point a readerwill be confused since we are discussing Dale and Reiter bith for generation of scenes and now for the generation of strings. We should have clarified the difference earlier. Also, rather than referring to it as Dale and Reiter we should call it "the incremental algorithm" and add a reference at first mention.

I have a trouble of understanding and interpreting these results. First the loss should be estimated on a test set, after the system has been trained. I'm not sure how you estaimate the baseline loss.

The sender uses the incremental algorithm?

But here we don't need an LSTM, we just want to identify one of the objects. The LSTM should only encode the inout message.

Reference back to when you reported this?

But receiver is not doing captioning? Is it incrementally identifying the objects as the sender's messages are coming in?

I'm not sure how teacher forcing is implemented, see my earlier comment

So the sender is generating descriptions following the GRE policy?

With the vocabulary size of 100.

Not sure. The difference is still very small, within a couple of %.

Very small decerase for such a large difference in vocabulary size compared to what we have seen earlier.

What is word-by-word?

Here it would be much better to have a loss curve to show differences in loss between the number of communication events, the same could also be done for accuracy. Then we would have a clearer picture of how vocabulary size affects learning. It could be that with 100 vocabulary there is actually a better performance than before the final cut-off point. Is this possible? How was the final cut-off point determined anyway?

Negative result: how can we then interpret that? What could we change? What did we learn from this?

But previously we have said that we are not going to study this game. But good that we have!

Not true: human knowledge is still reflected in the way the environment is structured.

Which contains human knowledge!

I'm not following this

Just thinking: would there be a more systematic way to compare all the models? Perhaps if you write the parameters in a table or a text that is parallel. Then you could simply say, modal A + these changes. It would be much clearer to the reader how all models are rerlated and it would be easier for the reader to follow.

Again, why LSTM, are we inceremtally identifying the image while we are processing each input word?

Just occured to me: how do we measure accuracy if we are predicting numbers (numeric prediction)?

But how could it do so if the visual data is randomly distribution. It must rely on the message as that is the only source of information.

Remind the reader what is the size of the image so that they can interprert the distance relatively to that.

I don't understand this

A bit gloomy result. Negative result is okay but then we need an explanation why this is so and have some ideas what to change. What we can learn from these results?

This identifies that the problem lies in how the sender encodes the messages; what is its policy to generate longer strings or reuse the word. Without a policy the sender will never be motivated to encode longer messages and therefore rely on single-word expressions.The second problem is the size of these networks. We should also test smaller embedding sizes than 10. Practice shows that the embedding sizes can be even several magnitues smaller than the vocabulary sizes, cf. the Bengio paper and word2vec.The reason why the system is not performing well on the pointing task is that it does not have the right features to learn from. It ahs visual information WHAT these objects are (what do they look like) but they do not have information WHERE these objects are,. John Kelleher and I wrote an opiiuon piece about the lack of spatial knowledge required to model spatial descriptions in CNNs. The same problem is probably occuring here. Replacing the features or adding geometric features that would communicate geometric relations that allow pointing would improve the task and also we would expect that a vocabulary would emerge such that some words are more biased towards visual features (to identify objects) and some more to geometric features (to locate them), hence WHAT and WHERE.

J. D. Kelleher and S. Dobnik. What is not where: the challenge of integrating spatial representations into deep learning architectures. In S. Dobnik and S. Lappin, editors, Proceedings of the Conference on Logic and Machine Learning in Natural Language (LaML 2017), Gothenburg, 12 –13 June, volume 1 of CLASP Papers in Computational Linguistics, pages 41–52, Gothenburg, Sweden, November 2017. Department of Philosophy, Linguistics and Theory of Science (FLOV), University of Gothenburg, CLASP, Centre for Language and Studies in Probability.https://gup.ub.gu.se/publication/262970?lang=enOverall, the thesis has a lot of potential if we could implement all this

