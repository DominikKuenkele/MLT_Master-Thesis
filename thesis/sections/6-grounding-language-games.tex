\section{Grounding referring expressions in language games}
\label{sec:language-games}
\cmtDK[inline]{'epochs', training/testing (success), optimizer, ...}
\cmtDK[inline]{max message length}

The experiments that are executed using language games have a similar structure as the experiments in the previous chapters, since those provided the basis for the language games.
% SD: But there is an importnat difference that now we have two neural netweorks, the sender and the receiver.
% DK: TODO

The language games in this research have an asymmetric setup.
One agent, the sender is shown some information and needs to generate a message.
This message is received by the second agent, the receiver.
The receiver needs to parse this message and combine it with the same information that the sender was presented with.
The receiver then makes a prediction, which is compared to the ground truth.
The game is set up prosocial, which means that both agents receive the same loss based on the receiver's prediction.
All weights of the agents are adapted in the same way.

The vocabulary that the sender can draw from to produce a message is made up of initially arbitrary symbols.
The meaning of these symbols is created as soon as the sender uses them in one of the message, and the receiver is able to use it to solve the task successfully.
Over time, specific meanings are reinforced and a language emerges.
The number of possible words, namely the size of the vocabulary can be varied.
% SD: But possibly you would describe the language game setup already before on the background or method chapter. I would move all text there and make the language games description much more detailed, also giving examples. What is happening in terms of interaction? How is this modelled with the neural networks? It is important to emphasise this difference.
% DK: TODO

First, we will discuss \emph{discrimination games}, because they have the simplest setup.
% SD: What is this setup?
% DK: TODO
Furthermore, other language games that research this topic use a very similar setup.
In the next step, we will describe \emph{caption generators} that are set up as a language game.
Here, the sender describes the scene, while the receiver needs to generate a caption.
In the last step, we try to lose as much human bias as possible and the models are trained on just 'pointing' towards the target object, by again predicting its center coordinates.
% SD: Why are we not testing it on all 3 games?
% SD: Not sure what you mean here? Why do we loose human bias? 
% DK: TODO

The languages that emerge are analyzed in two ways.
An analysis of the frequency of used symbols and messages, as well as an examination for which images and objects they are used indicates the structure of the language and meanings of the symbols.
In a second step, the language is compared to several referring expressions in natural language.
By doing this, it can be seen if the models learned to use similar ways of referring to objects as humans do, or if they rely on different approaches.

\cmtDK[inline]{discuss training/epochs/batches, vocab size, message length, gs temperature}
