\section{Experimental Setup}
\label{sec:exp-setup}

\cmtDK[inline]{15 pages}
\cmtDK[inline]{\\
    - what is ambiguity? \\
    - control over dataset \\
    - target object/target group/non-target group: \\
    ---- 1. how many objects? \\
    ---- 2. how many shared attributes? \\
    ---- 3. which shared attributes? \\
    ---- 4. level of ambiguity \\
    - as close to CLEVR as possible \\
    ---- image size \\
    ---- number (10.000) \\
    ---- between 3 to 10 objects ??? \\
    - unambigous dataset: \\
    ---- target group = non-target group \\
    ---- all attributes shared, except for the color \\
    ---- \ \  --> describable with one 'word'
}
In this section I will describe which experiments were conducted to answer the \cmtDK{?}{questions} from the previous sections as well as the setup of these experiments. This will be done in two parts. The first part contains the creation of the datasets that will be used in the experiments. In the second part I will go deeper into discussing each experiment, including the necessity of the experiment, the specifities of the dataset, as well as the final architecture of the models.

\subsection{Creation of the dataset}
The basis for my dataset is the CLEVR dataset (\cite{Johnson2016}). This dataset includes 3D-generated images depicting scenes with different kind of objects. Each of these objects has different combinations attributes, such as \emph{shape}, \emph{color}, \emph{size} and \emph{material}. The possible values of these attributes are listed in Table \ref{tab:clevr-attributes}. Three to ten objects are randomly placed into the scene and assigned with random attributes. To enhance realism and reduce ambiguity, objects do not intersect, have a certain distance from each other, and are at least partially visible.
Figure \ref{fig:clevr-example} shows an example of a generated image in the CLEVR dataset.

\begin{table}[h]
    \centering
    \begin{tabular}{cccc}
        \toprule
        \textbf{ shape } & \textbf{ color } & \textbf{ size } & \textbf{ material } \\
        cube             & gray             & small           & rubber              \\
        sphere           & red              & large           & metal               \\
        cylinder         & blue                                                     \\
                         & green                                                    \\
                         & brown                                                    \\
                         & purple                                                   \\
                         & cyan                                                     \\
                         & yellow                                                   \\
        \bottomrule
    \end{tabular}
    \caption{Attributes of objects in the CLEVR dataset}
    \label{tab:clevr-attributes}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\linewidth]{figures/CLEVR_example.png}
    \caption{Example of a generated image in the CLEVR dataset}
    \label{fig:clevr-example}
\end{figure}

Furthermore, the dataset contains information about each scene. This includes the selected attributes for each object as well as the exact position of the centers of all the objects, both 3D-coordinates in the 3D scene and 2D-coordinates in the final rendered image. In addition, simple spatial relations (in front of, behind, left, right) between the objects are calculated and stored. These are simply based on the 3D-coordinates of the objects from the perspective of the camera position.

\cmtDK[inline]{ambiguity}

This research investigates how agents communicate about the relations of objects seen in images. For that reason the original CLEVR dataset offers too little control over how the objects are created and in which relation they stand to each other. Following, I extended the source code to generated images for the CLEVR dataset.\footnote{The extended source code can be found on GitHub: \href{https://github.com/DominikKuenkele/MLT\_Master-Thesis\_clevr-dataset-gen}{https://github.com/DominikKuenkele/MLT\_Master-Thesis\_clevr-dataset-gen}} To simplify the generation, I only focused on the attributes \emph{shape}, \emph{size} and \emph{color}. The \emph{material} is always the same for all objects in a generated image. There were three main extensions to the code:

First, objects in the scene were separated into three categories: one \emph{target object}, objects in a \emph{target group} and \emph{distractor} objects. The target object is the main object in the scene that should be identified and communicated by the agents. All other objects and their relations are based on this target object. The target group contains similar objects to the target object. These are objects that the agents need to discriminate the target object from. Finally, the distractors are objects that add noise to the scene and should make it more complex and are expected to teach the agents more precise descriptions. The number of the objects in both group can be controlled.

In a second step, when generating the images it is possible to define the relations between \emph{target object}/\emph{target group} and \emph{target object}/\emph{distractors}. The relation is defined as \textbf{how many} attributes of the target object are identical with objects in the target group and distractors respectively. For example the target object is a \emph{small red cube}. If two attributes are shared between target object and target group, objects in the target group could include \emph{small blue cube}, \emph{big red cube} or \emph{small red sphere} but couldn't include another \emph{small red cube} or a \emph{small blue cylinder}. The number of shared attributes can also be set to a range to make the discrimination task more challenging.

Lastly, it is also possible to define exactly \textbf{which} attributes should be shared between the target object and the groups. For example, it can be defined to have the same size for objects in the target group, but have randomly selected different shapes and color. This allows a very controlled generation of relations between the objects in the scene. Figure \ref{fig:clevr-extended_example} shows one generated image with this extended source code. Here, the target object is the large purple cylinder. The target group contains four objects that share zero to a maximum of two attributes. It is not controlled, which attributes are shared (they are selected randomly). There are no distractor objects.

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\linewidth]{figures/CLEVR_extended_example.png}
    \caption{Example of a generated image with the extended code}
    \label{fig:clevr-extended_example}
\end{figure}

For all generated datasets in the following sections, the general constraints and settings are as close as possible to the original CLEVR dataset. The size of the generated images is 480x320 pixels. 10.000 images are created for each of the datasets. Each image contains a maximum of 10 objects, that are not intersecting, have the same minimum distance and are partially visible.

\subsection{Description of experiments}
