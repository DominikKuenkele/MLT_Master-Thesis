\subsection{Reference resolution}
\subsubsection*{Setup}
In the final experiments, the agents are tasked to produced and understand referring expressions only based on the implicit human knowledge present in the scenes, as for example the attributes of the objects.
Opposed to the previous experiments, explicit human language information as human referring expressions or one-hot encoded attributes are avoided.
By this, the agents can't just reuse these human referring expressions, but need to learn to generate them themselves during the language games just based on the visual input.
This is done similarly to the single model reference resolution task, described in section \ref{sec:reference_resolution}.
As before, the sender has the same architecture as in the previous language games, encoding the bounding boxes of all objects.

The receiver is tasked to solve two tasks.
First, in the \textbf{coordinate reference resolver}, the agents are tasked to predict the center coordinates of the target object (see Figure \ref{fig:coordinate_predictor_game_architecture}).
The receiver is the same in both setups and is based on the \emph{reference resolver} + referring expressions, described in section \ref{sec:reference_resolution}.
Again, instead of the included LSTM, the receiver uses the \emph{EGG} LSTM with the hidden size $h_r=500$ and $e_r=100$ to encode the message.
The dimensions of the coordinate predictor are fixed to $c=2048$, based on the previous results.
% SD: Just thinking: would there be a more systematic way to compare all the models? Perhaps if you write the parameters in a table or a text that is parallel. Then you could simply say, modal A + these changes. It would be much clearer to the reader how all models are rerlated and it would be easier for the reader to follow.
% DK: TODO
The euclidean distance between the resulting prediction of the center point and the true center of the target object is calculated and the weights of both agents are adapted accordingly.

\begin{figure}[ht]
    \centering
    \includegraphics[width=.7\linewidth]{figures/arch_coordinate_predictor_game.png}
    \caption{Simplified architecture of the masked coordinate predictor game}
    \label{fig:coordinate_predictor_game_architecture}
    % SD: Again, why LSTM, are we inceremtally identifying the image while we are processing each input word?
    % DK: because it's a message sequence of discrete symbols (done)
\end{figure}

Secondly, in the \textbf{attention reference resolver}, the agents learn to point towards regions in the image, which is based on the \emph{attention reference resolver} described in section \ref{sec:reference_resolution}.
As before, the LSTM is replaced with the \emph{EGG} LSTM to parse the sender's message with the hidden size $h_r=500$ and $e_r=100$.
The dot product is calculated between each encoded region of the image and the encoded message, and the $softmax$ function is applied subsequently.
The loss is calculated using \emph{binary cross entropy} between the predictions and ground truth regions.

\cmtDK[inline]{figure}

The experiments are conducted with the following hyperparameters: a learning rate of $2\times10^{-4}$, a temperature for the Gumbel-Softmax relaxation of 1 and \emph{Adam} \citep{Kingma2015} as optimizer.
The following values for the variables are compared:
\begin{itemize}
    \item $|V|$: 2, 10, 16, 50, 100
    \item $n$: 1, 2, 3, 4,  6
\end{itemize}

As in section \ref{sec:reference_resolution}, the agents are trained first on the \emph{CLEVR single} dataset to understand if they are capable of predicting locations in an image together.
In a next step, the \emph{Dale-2}, \emph{Dale-5} and \emph{CLEVR color} datasets are used to test if the agents are able to first communicate a target object and second describe the target object discriminatively with a small vocabulary.
The same metrics are used to evaluate the results as described in \ref{sec:reference_resolution}.

\subsection*{Results}
\begin{table}[ht]
    \centering
    \begin{tabular}{cc|cc|cc|cc}
        \toprule
                                      &         & \multicolumn{2}{c}{\textbf{Dale-2}} & \multicolumn{2}{c}{\textbf{Dale-5}} & \multicolumn{2}{c}{\textbf{CLEVR color}}                                                                                         \\  \cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
        $n$                           & $|V|$   & \textbf{loss}                       & \textbf{Acc.}                       & \textbf{loss}                            & \textbf{Acc.}               & \textbf{loss}             & \textbf{Acc.}               \\\midrule
        \multicolumn{2}{c|}{baseline} & {34,49} & {34,3\%}                            & {38,13}                             & {30,31\%}                                & {42,72}                     & {18,28\%}                                               \\\midrule
        {1}                           & {2}     & \textcolor{purple}{34,91}           & \textcolor{purple}{34,81\%}         & \textcolor{purple}{40,61}                & \textcolor{purple}{25,48\%} & {39,13}                   & {25\%}                      \\
        {1}                           & {10}    & \textcolor{purple}{35,31}           & {40,15\%}                           & \textcolor{purple}{38,73}                & \textcolor{purple}{28,12\%} & \textcolor{purple}{44,19} & {20,53\%}                   \\
        {1}                           & {16}    & \textcolor{purple}{35,4}            & {42,93\%}                           & \textcolor{purple}{39,58}                & \textcolor{purple}{26,17\%} & {39,64}                   & {24,18\%}                   \\
        {1}                           & {50}    & {32,64}                             & {42,01\%}                           & \textcolor{purple}{38,99}                & \textcolor{purple}{28,52\%} & \textcolor{purple}{42,61} & {20,79\%}                   \\
        {1}                           & {100}   & \textcolor{purple}{36,73}           & \textcolor{purple}{31,21\%}         & \textcolor{purple}{40,25}                & \textcolor{purple}{24,91\%} & \textcolor{purple}{41,5}  & {21,44\%}                   \\
        {2}                           & {2}     & \textcolor{purple}{33,76}           & {40,67\%}                           & {35,86}                                  & {31,73\%}                   & \textcolor{purple}{41,79} & {22,14\%}                   \\
        {2}                           & {10}    & {31,37}                             & {43,27\%}                           & {37,23}                                  & \textcolor{purple}{30,34\%} & \textcolor{purple}{42,53} & \textcolor{purple}{19,75\%} \\
        {2}                           & {16}    & {31,19}                             & {46,27\%}                           & \textcolor{purple}{39,38}                & \textcolor{purple}{28,21\%} & \textcolor{purple}{41,74} & {22,7\%}                    \\
        {2}                           & {50}    & \textcolor{purple}{33,66}           & {39,28\%}                           & \textcolor{purple}{39,62}                & \textcolor{purple}{25,35\%} & \textcolor{purple}{45,63} & \textcolor{purple}{17,93\%} \\
        {2}                           & {100}   & \textcolor{purple}{35,1}            & \textcolor{purple}{34,81\%}         & \textcolor{purple}{39,86}                & \textcolor{purple}{27,3\%}  & \textcolor{purple}{45,45} & \textcolor{purple}{19,05\%} \\
        {3}                           & {2}     & {32,99}                             & {41,15\%}                           & {37,76}                                  & \textcolor{purple}{30,99\%} & \textcolor{purple}{46,45} & {20,92\%}                   \\
        {3}                           & {10}    & \textcolor{purple}{33,74}           & {38,89\%}                           & \textcolor{purple}{40,71}                & \textcolor{purple}{26,48\%} & \textcolor{purple}{44,53} & \textcolor{purple}{19,01\%} \\
        {3}                           & {16}    & \textcolor{purple}{34,18}           & {38,98\%}                           & \textcolor{purple}{38,52}                & \textcolor{purple}{30,03\%} & \textcolor{purple}{41,22} & {21,53\%}                   \\
        {3}                           & {50}    & \textcolor{purple}{34,38}           & {38,93\%}                           & \textcolor{purple}{39,52}                & \textcolor{purple}{26,74\%} & {38,4}                    & {24,87\%}                   \\
        {3}                           & {100}   & \textcolor{purple}{33,16}           & {40,19\%}                           & \textcolor{purple}{39,75}                & \textcolor{purple}{27,78\%} & \textcolor{purple}{41,29} & {21,83\%}                   \\
        {4}                           & {2}     & \textcolor{purple}{33,48}           & {40,36\%}                           & \textcolor{purple}{38,28}                & \textcolor{purple}{30,9\%}  & \textcolor{purple}{42,53} & {22,4\%}                    \\
        {4}                           & {10}    & \textcolor{purple}{35,01}           & {39,97\%}                           & \textcolor{purple}{40,88}                & \textcolor{purple}{26,39\%} & \textcolor{purple}{42,33} & {20,44\%}                   \\
        {4}                           & {16}    & \textcolor{purple}{35,59}           & \textcolor{purple}{35,59\%}         & \textcolor{purple}{37,91}                & {31,86\%}                   & \textcolor{purple}{41,9}  & {20,27\%}                   \\
        {4}                           & {50}    & \textcolor{purple}{34,99}           & {36,68\%}                           & \textcolor{purple}{40,89}                & \textcolor{purple}{23,26\%} & \textcolor{purple}{43,77} & \textcolor{purple}{18,36\%} \\
        {4}                           & {100}   & \textcolor{purple}{35,46}           & {37,46\%}                           & \textcolor{purple}{38,53}                & \textcolor{purple}{29,08\%} & {40,6}                    & {21,22\%}                   \\
        {6}                           & {2}     & \textcolor{purple}{34,97}           & {37,28\%}                           & \textcolor{purple}{39,52}                & \textcolor{purple}{29,69\%} & {40,53}                   & {24,7\%}                    \\
        {6}                           & {10}    & {32,39}                             & {43,53\%}                           & \textcolor{purple}{38,61}                & \textcolor{purple}{28,26\%} & {40,84}                   & {21,74\%}                   \\
        {6}                           & {16}    & \textcolor{purple}{34,87}           & \textcolor{purple}{34,38\%}         & \textcolor{purple}{39,03}                & \textcolor{purple}{26,82\%} & {40,34}                   & {23,31\%}                   \\
        {6}                           & {50}    & \textcolor{purple}{33,85}           & {37,07\%}                           & \textcolor{purple}{39,39}                & \textcolor{purple}{27,17\%} & {40,8}                    & {22,09\%}                   \\
        {6}                           & {100}   & \textcolor{purple}{34,62}           & {40,02\%}                           & \textcolor{purple}{39,46}                & \textcolor{purple}{25,48\%} & \textcolor{purple}{42,53} & \textcolor{purple}{18,4\%}  \\
        \bottomrule
    \end{tabular}
    \caption{TODO: caption}
    \label{tab:results:coordinate-reference-resolver-game}
\end{table}

Table \ref{tab:results:coordinate-reference-resolver-game} shows the results of the coordinate reference resolver on different datasets.
The baselines show the predictions of the receiver without information of the sender, and correspond to the structural bias in the dataset.
When looking at how the agents fare with each dataset, not much general improvement can be seen.
Most of the configurations don't beat the baseline, independently of the shown dataset.
If they do, they can improve the mean distance of the predicted coordinates to the center of the target object only by 2 to 4 pixels.
In other words, the guesses are still not consistently precise on the target objects and the agents instead learn to point towards the center of the scene, as the single models in Section \ref{sec:referring_expression_generation}.
However, an influence on the accuracy can be identified.
The accuracy evaluates how many of the predictions are on the target object.
An increase in the accuracy with a stable mean distance corresponds shows a behavior where the agents make more correct predictions on the target objects, while the wrong predictions a further away from the target object.
As a conclusion, this can mean, that the agents' predictions, both correct and incorrect are more confident.
This seems to happen on the \emph{Dale-2} dataset.
The mean distance is not improved, but the accuracy reaches in some configurations up to 46,27\%.
This is an increase of around 13\% compared to the baseline.
While this value is still lower than a random guess between the objects in the scene, it shows, that some information is exchanged between the agents.
The meaning of the messages however doesn't seem to correspond to any attribute.

\begin{table}[ht]
    \centering
    \begin{tabular}{cc|c|c|c}
        \toprule
                                      &           & \multicolumn{1}{c}{\textbf{Dale-2}} & \multicolumn{1}{c}{\textbf{Dale-5}} & \multicolumn{1}{c}{\textbf{CLEVR color}} \\  \cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(lr){5-5}
        $n$                           & $|V|$     & \textbf{Probability mass}           & \textbf{Probability mass}           & \textbf{Probability mass}                \\\midrule
        \multicolumn{2}{c|}{baseline} & {62,16\%} & {49,61\%}                           & {41,68\%}                                                                      \\\midrule
        {1}                           & {2}       & {93,3\%}                            & {65,36\%}                           & \textcolor{purple}{39,22\%}              \\
        {1}                           & {10}      & {93,85\%}                           & \textcolor{purple}{46,97\%}         & \textcolor{purple}{40,1\%}               \\
        {1}                           & {16}      & {92,06\%}                           & {76,68\%}                           & \textcolor{purple}{36,43\%}              \\
        {1}                           & {50}      & {94\%}                              & {76,43\%}                           & \textcolor{purple}{39,97\%}              \\
        {1}                           & {100}     & {94,03\%}                           & \textcolor{purple}{54,14\%}         & \textcolor{purple}{40,27\%}              \\
        {2}                           & {2}       & {92,27\%}                           & \textcolor{purple}{52,15\%}         & \textcolor{purple}{33,64\%}              \\
        {2}                           & {10}      & \textbf{96,16\%}                    & {80,26\%}                           & \textcolor{purple}{36,53\%}              \\
        {2}                           & {16}      & {95,84\%}                           & \textbf{84,03\%}                    & \textcolor{purple}{39,65\%}              \\
        {2}                           & {50}      & {93,78\%}                           & \textcolor{purple}{52,24\%}         & \textcolor{purple}{39,56\%}              \\
        {2}                           & {100}     & {92,43\%}                           & \textcolor{purple}{53,23\%}         & \textcolor{purple}{37,68\%}              \\
        {3}                           & {2}       & {94,52\%}                           & \textcolor{purple}{51,97\%}         & \textcolor{purple}{37,09\%}              \\
        {3}                           & {10}      & {94,9\%}                            & \textcolor{purple}{53,47\%}         & \textcolor{purple}{38,24\%}              \\
        {3}                           & {16}      & {94,59\%}                           & \textbf{81,46\%}                    & \textbf{67,88\%}                         \\
        {3}                           & {50}      & {93,88\%}                           & {79,65\%}                           & \textcolor{purple}{40,36\%}              \\
        {3}                           & {100}     & {95,25\%}                           & \textcolor{purple}{48,52\%}         & \textcolor{purple}{42,55\%}              \\
        {4}                           & {2}       & {89,15\%}                           & \textcolor{purple}{51,98\%}         & \textcolor{purple}{39,68\%}              \\
        {4}                           & {10}      & \textbf{96,08\%}                    & \textcolor{purple}{48,03\%}         & \textbf{64,31\%}                         \\
        {4}                           & {16}      & {94,14\%}                           & \textcolor{purple}{49,81\%}         & \textcolor{purple}{40,84\%}              \\
        {4}                           & {50}      & \textbf{96,24\%}                    & \textcolor{purple}{48,79\%}         & \textcolor{purple}{43,61\%}              \\
        {4}                           & {100}     & {95,55\%}                           & \textcolor{purple}{49,65\%}         & \textcolor{purple}{42,85\%}              \\
        {6}                           & {2}       & \textcolor{purple}{59,68\%}         & \textcolor{purple}{53,57\%}         & \textcolor{purple}{38,43\%}              \\
        {6}                           & {10}      & \textcolor{purple}{63,46\%}         & \textbf{82,12\%}                    & \textcolor{purple}{40,11\%}              \\
        {6}                           & {16}      & {95,86\%}                           & \textcolor{purple}{50,71\%}         & \textcolor{purple}{40,61\%}              \\
        {6}                           & {50}      & {91,27\%}                           & \textcolor{purple}{52,55\%}         & \textcolor{purple}{40,21\%}              \\
        {6}                           & {100}     & \textcolor{purple}{60,27\%}         & \textcolor{purple}{46,92\%}         & \textcolor{purple}{41,98\%}              \\
        \bottomrule
    \end{tabular}
    \caption{TODO: caption}
    \label{TODO: label}
\end{table}

In the final setup, the agents are tasked with communicating objects with fewer infused human knowledge.
Table \ref{tab:results_dale_predictor_game} shows the results for the setup, in which the sender is pointed towards the target object with a human description based on the GRE algorithm.
Hereby, the 'CLEVR single' dataset acts as a baseline, to test if the agents are able to predict coordinates of objects at all.
In every configuration of the variables, the agents achieve a very high performance.
The worst average distance across the test dataset is 10 pixels, which still points onto an object.
Also the accuracy, which evaluates how many guesses of the receiver were pointing onto an object reflects this fact.
All configuration achieve an accuracy higher than 96,7\%.
% SD: Just occured to me: how do we measure accuracy if we are predicting numbers (numeric prediction)?
% DK: same as in the experiments with one model (done)
This aligns also with the results from the single neural models, where the average distance was similarly low.
In general, this shows that the agents are able to predict coordinates together.
However, the question arises if the message by the sender has actually an effect on the receivers' decision, or if the receiver learns to point towards the target coordinates on his own and the message is ignored.
Having a look at the transferred messages, it in fact shows that the receiver learns to point towards the target object on its own.
% SD: But how could it do so if the visual data is randomly distribution. It must rely on the message as that is the only source of information.
% DK: it does it in the same way as the single model above (TODO)
As in the experiment before, all communicated messages contain the same symbol independent of the input image.

\begin{table}[ht]
    \centering
    \begin{tabular}{ccc|ccc|ccc|ccc}
        \toprule
              &         &         & \multicolumn{3}{c}{\textbf{CLEVR single}} & \multicolumn{3}{c}{\textbf{Dale-2}} & \multicolumn{3}{c}{\textbf{Dale-5}}                                                                                                       \\\cmidrule(lr){4-6}\cmidrule(lr){7-9}\cmidrule(lr){10-12}
        $|V|$ & $h_{s}$ & $h_{r}$ & \textbf{Dist.}                            & \textbf{Acc.}                       & \textbf{length}                     & \textbf{Dist.} & \textbf{Acc.} & \textbf{length} & \textbf{Dist.} & \textbf{Acc.} & \textbf{length} \\\midrule
        {10}  & {10}    & {10}    & {10,1}                                    & {98,5\%}                            & {1}                                 & {36,5}         & {19,9\%}      & {1}             & {45,7}         & {14,4\%}      & {1}             \\
        {13}  & {10}    & {10}    & {6}                                       & {99\%}                              & {0}                                 & {38}           & {20,4\%}      & {1}             & {47,3}         & {10,8\%}      & {1}             \\
        {20}  & {10}    & {10}    & {9,7}                                     & {96,7\%}                            & {1}                                 & {37,3}         & {21,2\%}      & {1}             & {47,3}         & {11,3\%}      & {0}             \\
        {100} & {10}    & {10}    & {7,7}                                     & {98,4\%}                            & {1}                                 & {40,4}         & {21,7\%}      & {1}             & {45,4}         & {10,8\%}      & {1}             \\
        {100} & {100}   & {100}   & {7,5}                                     & {96,9\%}                            & {1}                                 & {40,1}         & {17,8\%}      & {1}             & {44,3}         & {11,8\%}      & {0}             \\
        \bottomrule
    \end{tabular}
    \caption{Results of the description coordinate predictor: $|V|$ are different vocabulary sizes and $h$ hidden sizes.}
    \label{tab:results_dale_predictor_game}
\end{table}

When the experiments are run on the 'Dale-2' dataset, the results are much worse.
For the \emph{description coordinate predictor}, the average distance ranges from 36,5 pixels to 40,4 pixels.
The configuration with a vocabulary size of only 10 symbols fares the best, while a vocabulary of 100 symbols produces the worst results.
% SD: Remind the reader what is the size of the image so that they can interprert the distance relatively to that.
% DK: TODO
Still, the accuracy shows that around 19,9\% to 21,7\% of the guesses are on the target object.
Here, the configurations with higher vocabulary sizes fare slightly better, but the differences are very small and likely due to other factors.

The results for 'Dale-5' dataset are even worse, but are comparable with the results with a single neural model.
Apparently, the agents are not able to communicate the target object, and the predictions by the receiver are general in the middle of the image, which results in an average distance of around 45 to 50 pixels.
The differences of the mean distances are not very significant in this range, to allow an analysis of the different configurations.
% SD: I don't understand this
% DK: TODO

\begin{table}[ht]
    \centering
    \begin{tabular}{cccc|ccc|ccc|ccc}
        \toprule
              &         &         &         & \multicolumn{3}{c}{\textbf{CLEVR single}} & \multicolumn{3}{c}{\textbf{Dale-2}} & \multicolumn{3}{c}{\textbf{Dale-5}}                                                                                                       \\\cmidrule(lr){5-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}
        $|V|$ & $h_{s}$ & $h_{r}$ & $e_{s}$ & \textbf{Dist.}                            & \textbf{Acc.}                       & \textbf{length}                     & \textbf{Dist.} & \textbf{Acc.} & \textbf{length} & \textbf{Dist.} & \textbf{Acc.} & \textbf{length} \\\midrule
        {10}  & {10}    & {10}    & {1024}  & {10,8}                                    & {93,1\%}                            & {1}                                 & {34,8}         & {24,3\%}      & {0}             & {44,3}         & {11,8\%}      & {1}             \\
        {10}  & {10}    & {10}    & {512}   & {9,3}                                     & {92\%}                              & {1}                                 & {36,3}         & {19,9\%}      & {0,7}           & {45,9}         & {12,7\%}      & {1}             \\
        {13}  & {10}    & {10}    & {1024}  & {7,8}                                     & {96,8\%}                            & {1}                                 & {36,3}         & {20,2\%}      & {0}             & {45,4}         & {11,4\%}      & {1}             \\
        {20}  & {10}    & {10}    & {1024}  & {6,6}                                     & {98,3\%}                            & {1}                                 & {37,8}         & {16,1\%}      & {1}             & {45,2}         & {11\%}        & {1}             \\
        {100} & {10}    & {10}    & {1024}  & {5,2}                                     & {98,5\%}                            & {1}                                 & {37,4}         & {20,1\%}      & {1}             & {43,6}         & {16,7\%}      & {1}             \\
        {100} & {100}   & {100}   & {1024}  & {12,5}                                    & {92,1\%}                            & {1}                                 & {36,5}         & {20,7\%}      & {1}             & {44,6}         & {12,7\%}      & {1}             \\
        \bottomrule
    \end{tabular}
    \caption{Results of the masked coordinate predictor: $|V|$ are different vocabulary sizes, $h$ hidden sizes and $e$ embedding sizes.}
    \label{tab:results_masked_predictor_game}
\end{table}

Finally looking at the setup, when using the masked image as an input shows that the results are similarly bad as when using the encoded captions.
This is easily explainable with the emerged language.
For both the experiments using the encoded captions and the experiments using masked images as input, no meaningful symbols are transferred.
% SD: A bit gloomy result. Negative result is okay but then we need an explanation why this is so and have some ideas what to change. What we can learn from these results?
% DK: TODO
Following, the receiver needs to solve the task alone and the different setups of the sender don't play any role in the overall success.
% SD: This identifies that the problem lies in how the sender encodes the messages; what is its policy to generate longer strings or reuse the word. Without a policy the sender will never be motivated to encode longer messages and therefore rely on single-word expressions.
% SD: The second problem is the size of these networks. We should also test smaller embedding sizes than 10. Practice shows that the embedding sizes can be even several magnitues smaller than the vocabulary sizes, cf. the Bengio paper and word2vec.
% SD: The reason why the system is not performing well on the pointing task is that it does not have the right features to learn from. It ahs visual information WHAT these objects are (what do they look like) but they do not have information WHERE these objects are,. John Kelleher and I wrote an opiiuon piece about the lack of spatial knowledge required to model spatial descriptions in CNNs. The same problem is probably occuring here. Replacing the features or adding geometric features that would communicate geometric relations that allow pointing would improve the task and also we would expect that a vocabulary would emerge such that some words are more biased towards visual features (to identify objects) and some more to geometric features (to locate them), hence WHAT and WHERE.
% SD: J. D. Kelleher and S. Dobnik. What is not where: the challenge of integrating spatial representations into deep learning architectures. In S. Dobnik and S. Lappin, editors, Proceedings of the Conference on Logic and Machine Learning in Natural Language (LaML 2017), Gothenburg, 12 –13 June, volume 1 of CLASP Papers in Computational Linguistics, pages 41–52, Gothenburg, Sweden, November 2017. Department of Philosophy, Linguistics and Theory of Science (FLOV), University of Gothenburg, CLASP, Centre for Language and Studies in Probability.https://gup.ub.gu.se/publication/262970?lang=en
% SD: Overall, the thesis has a lot of potential if we could implement all this
% DK: TODO

When comparing these results to the neural models in Section \ref{sec:reference_resolution} that are not part of a language game, it can be seen that the metrics are very similar for all datasets.
The model was already not able to solve the task without the increased complexity of a language game.
This therefore indicates that the challenge for the agents doesn't lie in grounding the extracted features in new symbols, but already in extracting the features in the first place.